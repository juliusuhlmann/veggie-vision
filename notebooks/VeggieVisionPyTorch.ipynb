{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class VeggieDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images  # Should be in (batch, channels, height, width)\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.transpose(self.images[idx], (1, 2, 0))  # Convert (C, H, W) -> (H, W, C)\n",
    "        image = Image.fromarray((image * 255).astype(np.uint8))  # Convert to PIL Image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32).squeeze()\n",
    "        return image, label\n",
    "\n",
    "# Model class\n",
    "class VeggieVision(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VeggieVision, self).__init__()\n",
    "        self.base_model = models.mobilenet_v2(pretrained=True)\n",
    "        self.base_model.features.requires_grad = False  # Freeze base layers\n",
    "        self.fc1 = nn.Linear(self.base_model.last_channel * 7 * 7, 128)  # Adjust input size here\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model.features(x)\n",
    "        x = x.reshape(x.size(0), -1)  # Use reshape\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)  # Linear output for regression\n",
    "        return x\n",
    "\n",
    "# Data transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(360),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.9, 1.1)),\n",
    "    transforms.ColorJitter(brightness=0.2),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Helper function to prepare data loaders\n",
    "def prepare_data_loaders(X, y, validation_split=0.2, batch_size=32):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=validation_split, random_state=42)\n",
    "    train_dataset = VeggieDataset(X_train, y_train, transform=train_transform)\n",
    "    val_dataset = VeggieDataset(X_val, y_val, transform=test_transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Model initialization with scheduler setup\n",
    "def initialize_model(device, learning_rate=1e-4):\n",
    "    model = VeggieVision().to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Set up a scheduler that reduces the learning rate on plateau of validation loss\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "    \n",
    "    return model, criterion, optimizer, scheduler\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def training_loop(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs=20, print_freq=1):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        # Train the model\n",
    "        train_loss = train(model, train_loader, criterion, optimizer, device, print_freq=print_freq)\n",
    "        \n",
    "        # Validate the model\n",
    "        val_loss = validate(model, val_loader, criterion, device, print_freq=print_freq)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Step the scheduler based on the validation loss\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "\n",
    "# Fine-tuning setup\n",
    "def setup_fine_tuning(model, optimizer, learning_rate=1e-5, n_unfreeze_layers=1):\n",
    "    fine_tune(model, n_unfreeze_layers)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return optimizer\n",
    "\n",
    "# Fine-tuning loop\n",
    "def fine_tuning_loop(model, train_loader, val_loader, criterion, optimizer, device, fine_tune_epochs=2):\n",
    "    for epoch in range(fine_tune_epochs):\n",
    "        train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss = validate(model, val_loader, criterion, device)\n",
    "        print(f\"Fine-tuning Epoch {epoch+1}/{fine_tune_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device, print_freq=1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_batches = len(dataloader)\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        # Print batch loss every `print_freq` batches\n",
    "        if (batch_idx + 1) % print_freq == 0:\n",
    "            print(f\"Batch {batch_idx+1}/{total_batches}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "def validate(model, dataloader, criterion, device, print_freq=1):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total_batches = len(dataloader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            # Print batch loss every `print_freq` batches\n",
    "            if (batch_idx + 1) % print_freq == 0:\n",
    "                print(f\"Validation Batch {batch_idx+1}/{total_batches}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "# Fine-tuning function\n",
    "def fine_tune(model, n_unfreeze_layers=1):\n",
    "    layers = list(model.base_model.features.children())\n",
    "    for layer in layers[-n_unfreeze_layers:]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "def predict(model, X, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    # Ensure X is a torch tensor with float32 data type and the correct shape\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "    \n",
    "    # Add a batch dimension if predicting on a single image\n",
    "    if len(X.shape) == 3:  # If X is (channels, height, width), add batch dimension\n",
    "        X = X.unsqueeze(0)\n",
    "    \n",
    "    # Send the batch of images to the specified device\n",
    "    X = X.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(X)\n",
    "        predictions = outputs.squeeze().cpu().numpy()  # Convert to numpy for easier handling\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def evaluate(model, X_test, y_test, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    # Generate predictions for the entire test set using the predict function\n",
    "    predictions = predict(model, X_test, device)\n",
    "    \n",
    "    # Calculate Mean Squared Error using the true labels and predictions\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    print(f'Mean Squared Error on Test Set: {mse:.4f}')\n",
    "    \n",
    "    return mse, np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "X = np.load(\"../data/processed_data/X.npy\")\n",
    "y = np.load(\"../data/processed_data/y.npy\")\n",
    "\n",
    "# Convert X from (batch, height, width, channel) to (batch, channel, height, width)\n",
    "X = np.transpose(X, (0, 3, 1, 2)).astype(np.float32)\n",
    "y = y.reshape(-1).astype(np.float32)\n",
    "\n",
    "# Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juliu\\.conda\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\juliu\\.conda\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\juliu\\.conda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "# Prepare data loaders\n",
    "train_loader, val_loader = prepare_data_loaders(X_train, y_train, 0.2, 32)\n",
    "    \n",
    "# Initialize model, loss function, and optimizer\n",
    "model, criterion, optimizer, scheduler = initialize_model(device, 1e-3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Batch 1/15, Loss: 8410.4902\n",
      "Batch 2/15, Loss: 11357.5312\n",
      "Batch 3/15, Loss: 4908.5093\n",
      "Batch 4/15, Loss: 2981.1782\n",
      "Batch 5/15, Loss: 3605.3391\n",
      "Batch 6/15, Loss: 6797.9438\n",
      "Batch 7/15, Loss: 3585.3994\n",
      "Batch 8/15, Loss: 1787.7040\n",
      "Batch 9/15, Loss: 2068.0012\n",
      "Batch 10/15, Loss: 2467.5237\n",
      "Batch 11/15, Loss: 216807.7500\n",
      "Batch 12/15, Loss: 4716.5762\n",
      "Batch 13/15, Loss: 3977.4531\n",
      "Batch 14/15, Loss: 4598.2803\n",
      "Batch 15/15, Loss: 1248.3616\n",
      "Validation Batch 1/4, Loss: 2844.3254\n",
      "Validation Batch 2/4, Loss: 2243.9270\n",
      "Validation Batch 3/4, Loss: 2579.0349\n",
      "Validation Batch 4/4, Loss: 2285.7124\n",
      "Epoch 1/2, Train Loss: 19104.8150, Validation Loss: 2507.2919\n",
      "Epoch 2/2\n",
      "Batch 1/15, Loss: 1672.2075\n",
      "Batch 2/15, Loss: 2899.3528\n",
      "Batch 3/15, Loss: 3853.6045\n",
      "Batch 4/15, Loss: 2941.9451\n",
      "Batch 5/15, Loss: 4671.2749\n",
      "Batch 6/15, Loss: 3400.8130\n",
      "Batch 7/15, Loss: 2164.2004\n",
      "Batch 8/15, Loss: 3621.0308\n",
      "Batch 9/15, Loss: 192725.0625\n",
      "Batch 10/15, Loss: 5711.9707\n",
      "Batch 11/15, Loss: 7732.3892\n",
      "Batch 12/15, Loss: 7409.9111\n",
      "Batch 13/15, Loss: 3984.1997\n",
      "Batch 14/15, Loss: 3210.1306\n",
      "Batch 15/15, Loss: 2829.0957\n",
      "Validation Batch 1/4, Loss: 3982.3503\n",
      "Validation Batch 2/4, Loss: 3277.3840\n",
      "Validation Batch 3/4, Loss: 4252.5464\n",
      "Validation Batch 4/4, Loss: 5802.6343\n",
      "Epoch 2/2, Train Loss: 16971.5028, Validation Loss: 4190.1564\n"
     ]
    }
   ],
   "source": [
    "# Training phase\n",
    "training_loop(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning phase\n",
    "#optimizer = setup_fine_tuning(model, optimizer, learning_rate=1e-5, n_unfreeze_layers=1)\n",
    "#fine_tuning_loop(model, train_loader, val_loader, criterion, optimizer, device, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Test Set: 4503.1807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4503.1807,\n",
       " array([ 85.92173 , 113.28998 ,  73.26694 , 134.63368 , 138.56113 ,\n",
       "         91.371   ,  82.55229 , 125.32519 , 118.35986 ,  91.33328 ,\n",
       "        115.21034 ,  91.2572  ,  92.06623 , 115.58621 , 123.13333 ,\n",
       "        113.234406,  87.81117 , 153.18767 ,  79.40527 ,  87.22033 ,\n",
       "        102.14139 ,  93.86496 , 145.17398 , 114.56839 ,  77.53392 ,\n",
       "         96.780785,  91.20685 , 135.11786 ,  99.32752 , 100.36951 ,\n",
       "         85.12936 , 109.99549 , 113.77382 ,  84.85522 ,  97.80151 ,\n",
       "        140.19856 ,  89.58275 , 106.668175, 103.25375 ,  86.25515 ,\n",
       "         88.3547  , 131.36534 , 105.71373 ,  88.763466,  98.011765,\n",
       "         88.961716, 108.523575,  94.915726,  83.93364 , 110.656494,\n",
       "        132.17863 ,  80.69373 , 101.839035, 103.32461 ,  92.53502 ,\n",
       "         83.64021 , 112.166275, 100.4583  , 141.89357 ,  82.50578 ,\n",
       "         87.4367  ,  89.91699 , 118.32826 ,  94.778656,  85.295   ,\n",
       "         93.02997 ,  79.36111 ,  88.74667 ,  94.9993  ,  99.486534,\n",
       "         93.77359 , 100.85672 ,  87.079895, 108.75188 , 114.67103 ,\n",
       "         92.64513 , 128.36778 , 110.922134,  92.82073 ,  84.93928 ,\n",
       "         88.783226,  94.66594 ,  87.22358 , 134.9879  ,  78.106445,\n",
       "         93.98424 ,  93.677246, 121.49515 , 101.58746 ,  79.23114 ,\n",
       "         92.01374 ,  98.13603 , 132.00839 , 101.417046,  85.92734 ,\n",
       "         93.73004 , 116.32122 ,  98.26345 , 127.88045 ,  83.10553 ,\n",
       "         99.45403 , 102.456604,  90.46849 ,  98.7251  ,  81.33952 ,\n",
       "        100.04512 ,  85.47807 ,  78.07885 , 100.96309 ,  95.75548 ,\n",
       "        102.06419 , 121.36499 ,  91.7193  ,  82.43346 , 129.48839 ,\n",
       "        147.69652 ,  83.33649 ,  90.84938 ,  89.37222 , 114.74568 ,\n",
       "        100.27467 ,  86.725426,  83.21601 ,  94.83009 ,  89.55939 ,\n",
       "        139.13145 , 121.91293 ,  90.81603 ,  85.060005,  98.14083 ,\n",
       "         88.447876, 116.60437 , 163.41151 ,  90.9388  ,  95.664955,\n",
       "         72.56209 ,  97.49488 , 133.20074 , 117.55213 , 102.89653 ,\n",
       "        123.39401 ,  86.34592 , 108.56778 , 114.813385,  91.13313 ,\n",
       "         99.68396 ,  86.74501 ], dtype=float32))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation phase\n",
    "evaluate(model, X_test, y_test, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
