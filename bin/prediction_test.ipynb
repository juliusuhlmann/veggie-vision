{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"../data/processed_data/X.npy\")\n",
    "y = np.load(\"../data/processed_data/y.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.image.rgb_to_grayscale(X).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "33/33 [==============================] - 7s 208ms/step - loss: 22496.4219 - mae: 82.1105 - val_loss: 5107.5332 - val_mae: 61.9608\n",
      "Epoch 2/20\n",
      "33/33 [==============================] - 7s 204ms/step - loss: 18940.0195 - mae: 70.1732 - val_loss: 5263.6299 - val_mae: 61.8764\n",
      "Epoch 3/20\n",
      "33/33 [==============================] - 7s 198ms/step - loss: 18667.5195 - mae: 68.0882 - val_loss: 5721.4692 - val_mae: 66.3710\n",
      "Epoch 4/20\n",
      "33/33 [==============================] - 7s 200ms/step - loss: 17665.9297 - mae: 68.7194 - val_loss: 6454.5264 - val_mae: 69.4622\n",
      "Epoch 5/20\n",
      "33/33 [==============================] - 7s 199ms/step - loss: 17905.5371 - mae: 69.5022 - val_loss: 5237.3579 - val_mae: 62.4133\n",
      "Epoch 6/20\n",
      "33/33 [==============================] - 7s 199ms/step - loss: 17304.1914 - mae: 65.7112 - val_loss: 5067.5083 - val_mae: 61.8846\n",
      "Epoch 7/20\n",
      "33/33 [==============================] - 7s 198ms/step - loss: 16227.3447 - mae: 62.9023 - val_loss: 5709.4985 - val_mae: 65.8790\n",
      "Epoch 8/20\n",
      "33/33 [==============================] - 7s 198ms/step - loss: 15318.3115 - mae: 68.0871 - val_loss: 5485.6987 - val_mae: 64.5452\n",
      "Epoch 9/20\n",
      "33/33 [==============================] - 7s 198ms/step - loss: 12687.2158 - mae: 62.7831 - val_loss: 5581.1919 - val_mae: 63.4754\n",
      "Epoch 10/20\n",
      "33/33 [==============================] - 7s 198ms/step - loss: 12762.2305 - mae: 61.2907 - val_loss: 6462.3428 - val_mae: 69.7966\n",
      "Epoch 11/20\n",
      "33/33 [==============================] - 7s 198ms/step - loss: 11156.6719 - mae: 59.9805 - val_loss: 5567.0088 - val_mae: 63.7787\n",
      "Epoch 12/20\n",
      "33/33 [==============================] - 7s 204ms/step - loss: 9312.4385 - mae: 57.9822 - val_loss: 6426.7231 - val_mae: 68.9296\n",
      "Epoch 13/20\n",
      "33/33 [==============================] - 7s 201ms/step - loss: 7214.7764 - mae: 56.4795 - val_loss: 6289.7314 - val_mae: 68.3491\n",
      "Epoch 14/20\n",
      "33/33 [==============================] - 7s 203ms/step - loss: 6756.4058 - mae: 56.8071 - val_loss: 6948.1484 - val_mae: 62.1380\n",
      "Epoch 15/20\n",
      "33/33 [==============================] - 7s 221ms/step - loss: 5356.4858 - mae: 54.6513 - val_loss: 6001.6328 - val_mae: 62.9344\n",
      "Epoch 16/20\n",
      "33/33 [==============================] - 7s 213ms/step - loss: 4172.4756 - mae: 50.5545 - val_loss: 5930.2056 - val_mae: 65.7334\n",
      "Epoch 17/20\n",
      "33/33 [==============================] - 7s 209ms/step - loss: 4527.0347 - mae: 51.1810 - val_loss: 6683.8062 - val_mae: 62.2004\n",
      "Epoch 18/20\n",
      "33/33 [==============================] - 7s 212ms/step - loss: 3409.2839 - mae: 46.8192 - val_loss: 5605.5356 - val_mae: 61.2471\n",
      "Epoch 19/20\n",
      "33/33 [==============================] - 7s 203ms/step - loss: 3476.6536 - mae: 46.0796 - val_loss: 5958.1914 - val_mae: 64.6217\n",
      "Epoch 20/20\n",
      "33/33 [==============================] - 7s 200ms/step - loss: 3344.9983 - mae: 43.9673 - val_loss: 5661.6528 - val_mae: 59.1484\n",
      "5/5 [==============================] - 0s 63ms/step - loss: 5574.1895 - mae: 55.0208\n",
      "Epoch 1/50\n",
      "33/33 [==============================] - 7s 200ms/step - loss: 22027.0664 - mae: 78.6465 - val_loss: 5066.9448 - val_mae: 62.7391\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 6s 195ms/step - loss: 19424.4180 - mae: 72.4698 - val_loss: 5692.6792 - val_mae: 60.6182\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 6s 192ms/step - loss: 19954.4121 - mae: 69.8639 - val_loss: 5350.5034 - val_mae: 60.3435\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 6s 192ms/step - loss: 17944.3281 - mae: 67.5548 - val_loss: 4983.9336 - val_mae: 62.3140\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 6s 193ms/step - loss: 19326.7793 - mae: 71.1412 - val_loss: 5149.1938 - val_mae: 61.1020\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 6s 192ms/step - loss: 18902.0391 - mae: 69.8949 - val_loss: 5006.4854 - val_mae: 61.6232\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 7s 198ms/step - loss: 16863.8398 - mae: 68.9678 - val_loss: 5447.2197 - val_mae: 61.1482\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 7s 201ms/step - loss: 16427.9414 - mae: 65.0919 - val_loss: 5053.0591 - val_mae: 62.0627\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 7s 201ms/step - loss: 15968.7656 - mae: 65.4146 - val_loss: 6395.2207 - val_mae: 69.9522\n",
      "5/5 [==============================] - 0s 60ms/step - loss: 5319.5840 - mae: 61.7703\n",
      "Mean Absolute Error on the test set: 61.77033996582031\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Train-test split (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a simpler CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# First Convolutional Layer\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(224, 224, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Second Convolutional Layer\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten layer to convert 2D to 1D\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dense layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Dropout to prevent overfitting\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Output layer for regression (1 unit)\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "model.compile(optimizer=Adam(learning_rate=0.0005), loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=16, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Train-test split (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a simpler CNN model with regularization and dropout\n",
    "model = Sequential()\n",
    "\n",
    "# First Convolutional Layer with L2 Regularization\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(224, 224, 1), kernel_regularizer=l2(0.001)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Second Convolutional Layer with L2 Regularization\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten layer to convert 2D to 1D\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dense layer with L2 Regularization\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "\n",
    "# Increased Dropout to prevent overfitting\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer for regression (1 unit)\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "model.compile(optimizer=Adam(learning_rate=0.0005), loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Add EarlyStopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Mean Absolute Error on the test set: {mae}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
